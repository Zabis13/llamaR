CXX_STD = CXX17

# Enable C99 format specifiers (%zu, %zd) on MinGW
PKG_CPPFLAGS_WIN = -D__USE_MINGW_ANSI_STDIO=1

# Get ggmlR paths
GGMLR_INCLUDE = $(shell "$(R_HOME)/bin/Rscript" -e "cat(system.file('include', package='ggmlR'))")
GGMLR_LIB = $(shell "$(R_HOME)/bin/Rscript" -e "cat(system.file('lib', package='ggmlR'))")

# Include paths
PKG_CPPFLAGS = -I. -I$(GGMLR_INCLUDE) -DGGML_USE_CPU $(PKG_CPPFLAGS_WIN)

# Compiler flags
PKG_CXXFLAGS = $(SHLIB_OPENMP_CXXFLAGS)

# Link with ggmlR static library
PKG_LIBS = -L$(GGMLR_LIB) -lggml $(SHLIB_OPENMP_CXXFLAGS) -lpthread -lm

# llama.cpp sources
SOURCES_CPP = \
  llama.cpp \
  llama-adapter.cpp \
  llama-arch.cpp \
  llama-batch.cpp \
  llama-chat.cpp \
  llama-context.cpp \
  llama-cparams.cpp \
  llama-grammar.cpp \
  llama-graph.cpp \
  llama-hparams.cpp \
  llama-impl.cpp \
  llama-io.cpp \
  llama-kv-cache.cpp \
  llama-kv-cache-iswa.cpp \
  llama-memory.cpp \
  llama-memory-hybrid.cpp \
  llama-memory-hybrid-iswa.cpp \
  llama-memory-recurrent.cpp \
  llama-mmap.cpp \
  llama-model.cpp \
  llama-model-loader.cpp \
  llama-model-saver.cpp \
  llama-quant.cpp \
  llama-sampling.cpp \
  llama-vocab.cpp \
  unicode.cpp \
  unicode-data.cpp \
  r_llama_interface.cpp

# Model implementations
MODELS_CPP = $(wildcard models/*.cpp)

OBJECTS = $(SOURCES_CPP:.cpp=.o) $(MODELS_CPP:.cpp=.o)

# Force include R compatibility header for files using stderr/stdout
# This redirects fprintf(stderr,...) to REprintf(...) for CRAN compliance
llama-grammar.o: PKG_CPPFLAGS += -include r_llama_compat.h
llama-impl.o: PKG_CPPFLAGS += -include r_llama_compat.h
llama-quant.o: PKG_CPPFLAGS += -include r_llama_compat.h
unicode.o: PKG_CPPFLAGS += -include r_llama_compat.h
