% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_generate}
\alias{llama_generate}
\title{Generate text from a prompt}
\usage{
llama_generate(
  ctx,
  prompt,
  max_new_tokens = 256L,
  temp = 0.8,
  top_k = 50L,
  top_p = 0.9,
  seed = 42L
)
}
\arguments{
\item{ctx}{Context handle returned by [llama_new_context]}

\item{prompt}{Character string prompt}

\item{max_new_tokens}{Maximum number of tokens to generate}

\item{temp}{Sampling temperature. 0 = greedy decoding.}

\item{top_k}{Top-K filtering (0 = disabled)}

\item{top_p}{Top-P (nucleus) filtering (1.0 = disabled)}

\item{seed}{Random seed for sampling}
}
\value{
Character string with generated text
}
\description{
Tokenizes the prompt, runs the full autoregressive decode loop with sampling,
and returns the generated text (excluding the original prompt).
}
\examples{
\dontrun{
model <- llama_load_model("model.gguf", n_gpu_layers = -1L)
ctx <- llama_new_context(model, n_ctx = 2048L)

# Basic generation
result <- llama_generate(ctx, "Once upon a time")
cat(result)

# Greedy decoding (deterministic)
result <- llama_generate(ctx, "The answer is", temp = 0)

# More creative output
result <- llama_generate(ctx, "Write a poem about R:",
                         max_new_tokens = 100L,
                         temp = 1.0, top_p = 0.95)
}
}
