% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_generate}
\alias{llama_generate}
\title{Generate text from a prompt}
\usage{
llama_generate(
  ctx,
  prompt,
  max_new_tokens = 256L,
  temp = 0.8,
  top_k = 50L,
  top_p = 0.9,
  seed = 42L,
  min_p = 0,
  typical_p = 1,
  repeat_penalty = 1,
  repeat_last_n = 64L,
  frequency_penalty = 0,
  presence_penalty = 0,
  mirostat = 0L,
  mirostat_tau = 5,
  mirostat_eta = 0.1,
  grammar = NULL
)
}
\arguments{
\item{ctx}{Context handle returned by [llama_new_context]}

\item{prompt}{Character string prompt}

\item{max_new_tokens}{Maximum number of tokens to generate}

\item{temp}{Sampling temperature. 0 = greedy decoding.}

\item{top_k}{Top-K filtering (0 = disabled)}

\item{top_p}{Top-P (nucleus) filtering (1.0 = disabled)}

\item{seed}{Random seed for sampling}

\item{min_p}{Min-P filtering threshold (0.0 = disabled)}

\item{typical_p}{Locally typical sampling threshold (1.0 = disabled)}

\item{repeat_penalty}{Repetition penalty (1.0 = disabled)}

\item{repeat_last_n}{Number of last tokens to penalize (0 = disabled, -1 = context size)}

\item{frequency_penalty}{Frequency penalty (0.0 = disabled)}

\item{presence_penalty}{Presence penalty (0.0 = disabled)}

\item{mirostat}{Mirostat sampling mode (0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)}

\item{mirostat_tau}{Mirostat target entropy (tau parameter)}

\item{mirostat_eta}{Mirostat learning rate (eta parameter)}

\item{grammar}{GBNF grammar string for constrained generation (NULL = disabled)}
}
\value{
A character scalar containing the generated text (excluding the
  original prompt).
}
\description{
Tokenizes the prompt, runs the full autoregressive decode loop with sampling,
and returns the generated text (excluding the original prompt).
}
\examples{
if (FALSE) {
model <- llama_load_model("model.gguf", n_gpu_layers = -1L)
ctx <- llama_new_context(model, n_ctx = 2048L)

# Basic generation
result <- llama_generate(ctx, "Once upon a time")
cat(result)

# Greedy decoding (deterministic)
result <- llama_generate(ctx, "The answer is", temp = 0)

# More creative output
result <- llama_generate(ctx, "Write a poem about R:",
                         max_new_tokens = 100L,
                         temp = 1.0, top_p = 0.95)

# With repetition penalty
result <- llama_generate(ctx, "List items:",
                         repeat_penalty = 1.1, repeat_last_n = 64L)

# JSON output with grammar
result <- llama_generate(ctx, "Output JSON:",
                         grammar = 'root ::= "{" "}" ')
}
}
