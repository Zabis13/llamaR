% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_new_context}
\alias{llama_new_context}
\title{Create an inference context}
\usage{
llama_new_context(model, n_ctx = 2048L, n_threads = 4L)
}
\arguments{
\item{model}{Model handle returned by [llama_load_model]}

\item{n_ctx}{Context window size (number of tokens). 0 means use the model's trained value.}

\item{n_threads}{Number of CPU threads to use}
}
\value{
An opaque context handle (ExternalPtr). Freed automatically on GC or via [llama_free_context].
}
\description{
Create an inference context
}
\examples{
\dontrun{
model <- llama_load_model("model.gguf")
ctx <- llama_new_context(model, n_ctx = 4096L, n_threads = 8L)
# ... use context for generation ...
llama_free_context(ctx)
llama_free_model(model)
}
}
