% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_load_model}
\alias{llama_load_model}
\title{Load a GGUF model file}
\usage{
llama_load_model(path, n_gpu_layers = 0L)
}
\arguments{
\item{path}{Path to the .gguf model file}

\item{n_gpu_layers}{Number of layers to offload to GPU (0 = CPU only, -1 = all)}
}
\value{
An external pointer (class \code{externalptr}) wrapping the loaded
  model. This handle is required by \code{\link{llama_new_context}},
  \code{\link{llama_model_info}}, and other model-level functions.
  Freed automatically by the garbage collector or manually via
  \code{\link{llama_free_model}}.
}
\description{
Load a GGUF model file
}
\examples{
if (FALSE) {
# Load model on CPU only
model <- llama_load_model("model.gguf")

# Load model with all layers on GPU
model <- llama_load_model("model.gguf", n_gpu_layers = -1L)

# Load model with first 10 layers on GPU
model <- llama_load_model("model.gguf", n_gpu_layers = 10L)
}
}
