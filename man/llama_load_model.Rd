% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_load_model}
\alias{llama_load_model}
\title{Load a GGUF model file}
\usage{
llama_load_model(path, n_gpu_layers = 0L)
}
\arguments{
\item{path}{Path to the .gguf model file}

\item{n_gpu_layers}{Number of layers to offload to GPU (0 = CPU only, -1 = all)}
}
\value{
An opaque model handle (ExternalPtr). Freed automatically on GC or via [llama_free_model].
}
\description{
Load a GGUF model file
}
\examples{
\dontrun{
# Load model on CPU only
model <- llama_load_model("model.gguf")

# Load model with all layers on GPU
model <- llama_load_model("model.gguf", n_gpu_layers = -1L)

# Load model with first 10 layers on GPU
model <- llama_load_model("model.gguf", n_gpu_layers = 10L)
}
}
