% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_load_model}
\alias{llama_load_model}
\title{Load a GGUF model file}
\usage{
llama_load_model(path, n_gpu_layers = 0L, devices = NULL)
}
\arguments{
\item{path}{Path to the .gguf model file}

\item{n_gpu_layers}{Number of layers to offload to GPU (0 = CPU only, -1 = all)}

\item{devices}{Character vector of device names or types to use for offloading.
\code{NULL} (default) uses all available devices. Use \code{"cpu"} for CPU-only,
\code{"gpu"} for first GPU, or specific device names from
\code{\link{llama_backend_devices}}. Multiple devices enable multi-GPU split.}
}
\value{
An external pointer (class \code{externalptr}) wrapping the loaded
  model. This handle is required by \code{\link{llama_new_context}},
  \code{\link{llama_model_info}}, and other model-level functions.
  Freed automatically by the garbage collector or manually via
  \code{\link{llama_free_model}}.
}
\description{
Load a GGUF model file
}
\examples{
\dontrun{
# Load model on CPU only
model <- llama_load_model("model.gguf")

# Load model with all layers on GPU
model <- llama_load_model("model.gguf", n_gpu_layers = -1L)

# Explicit CPU-only backend
model <- llama_load_model("model.gguf", devices = "cpu")

# Specific GPU device (see llama_backend_devices())
model <- llama_load_model("model.gguf", n_gpu_layers = -1L, devices = "Vulkan0")

# Multi-GPU: use two devices
model <- llama_load_model("model.gguf", n_gpu_layers = -1L,
                          devices = c("Vulkan0", "Vulkan1"))
}
}
