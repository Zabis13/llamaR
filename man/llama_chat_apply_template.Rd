% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_chat_apply_template}
\alias{llama_chat_apply_template}
\title{Apply chat template to messages}
\usage{
llama_chat_apply_template(
  messages,
  template = NULL,
  add_generation_prompt = TRUE
)
}
\arguments{
\item{messages}{List of messages, each with `role` and `content` elements.
Roles are typically "system", "user", "assistant".}

\item{template}{Template string (from [llama_chat_template]) or NULL to use default}

\item{add_generation_prompt}{Whether to add the assistant prompt prefix at the end}
}
\value{
A character scalar containing the formatted prompt string, ready
  to be passed to \code{\link{llama_generate}}.
}
\description{
Formats a conversation using the specified chat template.
This is essential for instruct/chat models to work correctly.
}
\examples{
\dontrun{
model <- llama_load_model("llama-3.2-instruct.gguf")
tmpl <- llama_chat_template(model)

messages <- list(
  list(role = "system", content = "You are a helpful assistant."),
  list(role = "user", content = "What is R?")
)

prompt <- llama_chat_apply_template(messages, template = tmpl)
cat(prompt)

ctx <- llama_new_context(model)
response <- llama_generate(ctx, prompt)
}
}
