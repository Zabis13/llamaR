% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_chat_template}
\alias{llama_chat_template}
\title{Get model's built-in chat template}
\usage{
llama_chat_template(model, name = NULL)
}
\arguments{
\item{model}{Model handle returned by [llama_load_model]}

\item{name}{Optional template name (NULL for default)}
}
\value{
A character scalar with the chat template string, or \code{NULL} if
  the model does not contain a built-in template.
}
\description{
Returns the chat template string embedded in the model file, if any.
Common templates include ChatML, Llama, Mistral, etc.
}
\examples{
if (FALSE) {
model <- llama_load_model("llama-3.2-instruct.gguf")
tmpl <- llama_chat_template(model)
cat(tmpl)
}
}
