% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_get_embeddings_ith}
\alias{llama_get_embeddings_ith}
\title{Get embeddings for the i-th token in the batch}
\usage{
llama_get_embeddings_ith(ctx, i)
}
\arguments{
\item{ctx}{Context handle returned by [llama_new_context]}

\item{i}{Integer index of the token (0-based, or negative for reverse indexing)}
}
\value{
A numeric vector of length \code{n_embd}.
}
\description{
Returns the embedding vector for a specific token position after a decode
call with embeddings enabled. Negative indices count from the end
(-1 = last token).
}
\examples{
\dontrun{
model <- llama_load_model("model.gguf")
ctx <- llama_new_context(model)
llama_generate(ctx, "Hello world", max_new_tokens = 1L)

# Get the embedding of the last decoded token
emb <- llama_get_embeddings_ith(ctx, -1L)
cat("Embedding dim:", length(emb), "\n")
}
}
