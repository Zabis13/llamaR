% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_numa_init}
\alias{llama_numa_init}
\title{Initialize NUMA optimization}
\usage{
llama_numa_init(strategy = "disabled")
}
\arguments{
\item{strategy}{NUMA strategy: \code{"disabled"} (default), \code{"distribute"},
\code{"isolate"}, \code{"numactl"}, or \code{"mirror"}.}
}
\value{
No return value, called for side effects.
}
\description{
Call once for better performance on NUMA systems.
}
\examples{
\dontrun{
# On multi-socket servers, distribute memory across NUMA nodes
# for better memory bandwidth during inference
llama_numa_init("distribute")

# Call before loading any models â€” affects all subsequent allocations
model <- llama_load_model("model.gguf", n_gpu_layers = 0L)
}
}
