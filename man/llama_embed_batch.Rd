% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llama.R
\name{llama_embed_batch}
\alias{llama_embed_batch}
\title{Batch embeddings for multiple texts}
\usage{
llama_embed_batch(ctx, texts)
}
\arguments{
\item{ctx}{Context handle returned by [llama_new_context]}

\item{texts}{Character vector of texts to embed}
}
\value{
A numeric matrix with \code{nrow = length(texts)} and
  \code{ncol = n_embd}.
}
\description{
Computes embeddings for a character vector of texts in a single decode pass
using per-sequence pooling. This is more efficient than calling
\code{\link{llama_embeddings}} in a loop when embedding many texts.
}
\details{
Requires a model that supports pooled embeddings (e.g. embedding
  models like nomic-embed, bge, etc.). The context must have enough capacity
  for the total number of tokens across all texts. Causal attention is
  automatically disabled during computation.
}
\examples{
if (FALSE) {
model <- llama_load_model("embedding-model.gguf")
ctx <- llama_new_context(model, n_ctx = 2048L)
llama_set_causal_attn(ctx, FALSE)

mat <- llama_embed_batch(ctx, c("hello world", "foo bar", "test"))
# mat is a 3 x n_embd matrix
}
}
